1、尝试了bold-driver策略在逐个epoch之间动态调节学习率：
比较epoch结束后的test loss与之前记录的最小loss，如果test loss降低了，则将lr增加一个小幅度（5%比如），然后存储ckpt。反之，test loss升高了，则还原ckpt，同时将lr大幅度降低（50%比如）。
但是测试结果显示，50个epoch后的准确率大约不到72%，反之，使用固定学习率（1e-3，使用Adam）的训练过程能够最终达到约77%的准确率。
会不会是因为bold-driver容易让训练过程陷入局部极值？

2、看来通常使用每经过若干epoch就收缩lr的方法是正确的，现在测试了每过10个epoch，将lr缩小一个量级的策略（初始1e-3），这样训练50个epoch，能够达到79.9%的正确率。