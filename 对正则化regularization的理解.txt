各种正则化是通过在loss函数中额外添加一个与网络参数（权重和偏移）相关的正则项来实现的，例如：
L1正则化：添加一个所有权重的L1范数：loss = loss + lambda*Sum(|wi|)/n
L2正则化：添加一个所有权重的L2范数：loss = loss + lambda*Sum(wi*wi)/2/n

原理：
loss函数上的额外项会在优化过程中被最小化，因此实际上正则化就是在限定网络参数的大小，起到抑制参数过度增长的作用。
在过拟合的情况下，为了适配训练集中所有局部特征，网络会过度复杂化，在某些局部形成过大的梯度，也就是部分参数的过度增长，因此正则化可以起到抑制过拟合的作用。
L1范数和L2范数的惩罚项由于对权重的导数不同，在训练中抑制权重的方式也有所不同