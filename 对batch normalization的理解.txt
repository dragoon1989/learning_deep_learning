batch normalization是配合mini-batch梯度下降法使用的，在每个batch的数据前向传播的过程中，为了消除由于网络参数的更新导致的每一层输入数据方差和偏移的影响，在每一层输入开始前对数据修正为接近(0, 1)正态分布。
最早提出这种方法时，是想要在每一层输出进行激活之前添加normalization，后来发现其作用更主要地是对各层的输入数据进行减方差操作，放在每一层输入之前、上一层激活之后的效果更好
batch normalization能够减少过拟合的影响，提升模型的鲁棒性