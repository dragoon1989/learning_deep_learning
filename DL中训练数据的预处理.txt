假设每个输入特征向量包括m个特征，不对它们进行任何处理的话，这些特征数据的分布、量级等都可能有很大的差别。

按照反向传播的过程来看，假设loss函数是通过一个简单的神经元得到的：
loss = L(f(Wx+b))
其中L是loss函数，f是激活函数。
则反向传播时，更新权值wi时的传播误差为：
dL/dwi = dL/df*f'*xi（f'是激活函数的导数）
如果特征i和特征j的分布相差太大，直接后果就是dL/dwi和dL/dwj可能有很大的不同，在最坏的情况下，二者之中一个非常大，一个非常小。这样的话，就很难控制学习率以及学习过程中loss的变化过程，很可能在某些特征方向上的传播非常快速，但在另一些特征上则误差传播很慢。或者在某些特征上虽然误差可以正常速率传播，但作为代价，在其他一些特征上的误差传播太快，很容易出现曲折的收敛过程，甚至不收敛。
因此，如果让各个特征数据能够具有大体相当的分布，误差传播的过程就会比较容易通过统一的学习率来控制。
区分几种不同类型的训练数据：
（1）训练集中的每个样本都具有m种特征，但是同一个样本m种特征的分布不同，而不同样本中相同特征数据的分布相同
（2）训练集中的每个样本只有1种特征，不同样本中的特征分布相同
（3）训练集中的每个样本只有1种特征，不同样本中的特征分布不同

（1）对于第一类数据，可以根据全体样本数据统计出m种数据各自的均值和方差
（2）对于第二类数据，可以根据单个样本或者所有样本统计出特征的均值和方差
（3）对于第三类数据，只能根据单个样本推测该样本中的特征均值和方差

1、零均值化：
将训练数据的特征数据转换为均值为零的分布，只需要对各个样本的特征分别减去对应的均值即可
2、标准化：
对训练数据进行零均值化后，再将各个样本的特征分别除以对应的标准差

此外，有时为了从有限的训练样本中获得尽可能多的可用训练数据，还会对数据进行增强，例如对输入的原始图片进行平移、旋转、翻转、调节明暗和对比度等。
